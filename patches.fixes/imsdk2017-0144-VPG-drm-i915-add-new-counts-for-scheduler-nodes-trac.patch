From 70e14838ec74f1f34ccfeb1f0002287bb38c4727 Mon Sep 17 00:00:00 2001
From: John Harrison <john.c.harrison@intel.com>
Date: Tue, 9 Aug 2016 15:33:04 +0800
Subject: [PATCH 144/147] [VPG]: drm/i915: add new counts for scheduler nodes tracking

By adding the new counts (flying and queued) for each node queue
and maintain them when changing node state, we don't need to iterate
through the node queue one by one to get the needed counts.

Signed-off-by: John Harrison <john.c.harrison@intel.com>
---
 drivers/gpu/drm/i915/i915_scheduler.c |   58 ++++++++++++++------------------
 drivers/gpu/drm/i915/i915_scheduler.h |    8 ++++
 2 files changed, 33 insertions(+), 33 deletions(-)

Index: current/drivers/gpu/drm/i915/i915_scheduler.c
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_scheduler.c
+++ current/drivers/gpu/drm/i915/i915_scheduler.c
@@ -209,7 +209,8 @@ int i915_scheduler_init(struct drm_devic
  * hung when execfinal() was called and thus the ring submission needs to be
  * retried later.
  */
-static void i915_scheduler_node_requeue(struct i915_scheduler_queue_entry *node)
+static void i915_scheduler_node_requeue(struct i915_scheduler *scheduler,
+					struct i915_scheduler_queue_entry *node)
 {
 	WARN_ON(!I915_SQS_IS_FLYING(node));
 
@@ -218,6 +219,8 @@ static void i915_scheduler_node_requeue(
 	node->status = I915_SQS_QUEUED;
 	trace_i915_scheduler_unfly(node->params.ring, node);
 	trace_i915_scheduler_node_state_change(node->params.ring, node);
+	scheduler->counts[node->params.ring->id].flying--;
+	scheduler->counts[node->params.ring->id].queued++;
 }
 
 /*
@@ -234,8 +237,11 @@ static void i915_scheduler_node_kill(str
 	if (I915_SQS_IS_FLYING(node)) {
 		trace_i915_scheduler_unfly(node->params.ring, node);
 		scheduler->stats[node->params.ring->id].kill_flying++;
-	} else
+		scheduler->counts[node->params.ring->id].flying--;
+	} else {
 		scheduler->stats[node->params.ring->id].kill_queued++;
+		scheduler->counts[node->params.ring->id].queued--;
+	}
 
 	node->status = I915_SQS_DEAD;
 	trace_i915_scheduler_node_state_change(node->params.ring, node);
@@ -263,6 +269,7 @@ static void i915_scheduler_node_fly(stru
 
 	trace_i915_scheduler_fly(ring, node);
 	trace_i915_scheduler_node_state_change(ring, node);
+	scheduler->counts[ring->id].flying++;
 
 	if (!(scheduler->flags[ring->id] & I915_SF_INTERRUPTS_ENABLED)) {
 		bool success = true;
@@ -273,19 +280,10 @@ static void i915_scheduler_node_fly(stru
 	}
 }
 
-static uint32_t i915_scheduler_count_flying(struct i915_scheduler *scheduler,
+static inline uint32_t i915_scheduler_count_flying(struct i915_scheduler *scheduler,
 					    struct intel_engine_cs *ring)
 {
-	struct i915_scheduler_queue_entry *node;
-	uint32_t flying = 0;
-
-	assert_scheduler_lock_held(scheduler);
-
-	for_each_scheduler_node(node, ring->id)
-		if (I915_SQS_IS_FLYING(node))
-			flying++;
-
-	return flying;
+	return scheduler->counts[ring->id].flying;
 }
 
 static void i915_scheduler_priority_bump_clear(struct i915_scheduler *scheduler)
@@ -431,6 +429,7 @@ static int i915_scheduler_pop_from_queue
 		best->status = I915_SQS_POPPED;
 
 		trace_i915_scheduler_node_state_change(ring, best);
+		scheduler->counts[ring->id].queued--;
 
 		ret = 0;
 	} else {
@@ -549,7 +548,7 @@ static int i915_scheduler_submit(struct
 			}
 
 			if (requeue) {
-				i915_scheduler_node_requeue(node);
+				i915_scheduler_node_requeue(scheduler, node);
 				/*
 				 * No point spinning if the ring is currently
 				 * unavailable so just give up and come back
@@ -725,19 +724,12 @@ static int i915_scheduler_queue_execbuff
 	return 0;
 }
 
-static uint32_t i915_scheduler_count_incomplete(struct i915_scheduler *scheduler)
+static inline uint32_t i915_scheduler_count_incomplete(struct i915_scheduler *scheduler)
 {
-	struct i915_scheduler_queue_entry *test;
 	int r, incomplete = 0;
 
-	for (r = 0; r < I915_NUM_RINGS; r++) {
-		for_each_scheduler_node(test, r) {
-			if (I915_SQS_IS_COMPLETE(test))
-				continue;
-
-			incomplete++;
-		}
-	}
+	for (r = 0; r < I915_NUM_RINGS; r++)
+		incomplete += scheduler->counts[r].queued + scheduler->counts[r].flying;
 
 	return incomplete;
 }
@@ -841,6 +833,7 @@ int i915_scheduler_queue_execbuffer(stru
 
 	trace_i915_scheduler_queue(ring, node);
 	trace_i915_scheduler_node_state_change(ring, node);
+	scheduler->counts[ring->id].queued++;
 
 	spin_unlock_irq(&scheduler->lock);
 
@@ -890,6 +883,7 @@ bool i915_scheduler_notify_request(struc
 	}
 
 	trace_i915_scheduler_node_state_change(req->ring, node);
+	scheduler->counts[req->ring->id].flying--;
 
 	spin_unlock_irqrestore(&scheduler->lock, flags);
 
@@ -1021,18 +1015,10 @@ static bool i915_scheduler_remove(struct
 				  struct list_head *remove)
 {
 	struct i915_scheduler_queue_entry *node, *node_next;
-	int flying = 0, queued = 0;
 	bool do_submit;
 
 	spin_lock_irq(&scheduler->lock);
 
-	for_each_scheduler_node(node, ring->id) {
-		if (I915_SQS_IS_QUEUED(node))
-			queued++;
-		else if (I915_SQS_IS_FLYING(node))
-			flying++;
-	}
-
 	INIT_LIST_HEAD(remove);
 	list_for_each_entry_safe(node, node_next, &scheduler->node_queue[ring->id], link) {
 		if (!I915_SQS_IS_COMPLETE(node))
@@ -1065,7 +1051,8 @@ static bool i915_scheduler_remove(struct
 	}
 
 	/* Launch more packets now? */
-	do_submit = (queued > 0) && (flying < scheduler->min_flying);
+	do_submit = scheduler->counts[ring->id].queued > 0 &&
+		    scheduler->counts[ring->id].flying < scheduler->min_flying;
 
 	trace_i915_scheduler_remove(ring, do_submit);
 
@@ -1390,6 +1377,11 @@ int i915_scheduler_query_stats(struct in
 		stats->counts[node->status]++;
 	}
 
+	if (stats->counts[I915_SQS_QUEUED] != scheduler->counts[ring->id].queued)
+		printk(KERN_ERR "%s:%d> \x1B[31;1mQueued count mis-match: %d vs %d!\x1B[0m\n", __func__, __LINE__, stats->counts[I915_SQS_QUEUED], scheduler->counts[ring->id].queued);
+	if (stats->counts[I915_SQS_FLYING] != scheduler->counts[ring->id].flying)
+		printk(KERN_ERR "%s:%d> \x1B[31;1mFlying count mis-match: %d vs %d!\x1B[0m\n", __func__, __LINE__, stats->counts[I915_SQS_FLYING], scheduler->counts[ring->id].flying);
+
 	spin_unlock_irq(&scheduler->lock);
 
 	return 0;
Index: current/drivers/gpu/drm/i915/i915_scheduler.h
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_scheduler.h
+++ current/drivers/gpu/drm/i915/i915_scheduler.h
@@ -112,11 +112,19 @@ struct i915_scheduler_stats {
 	uint32_t kill_queued;
 };
 
+struct i915_scheduler_node_states {
+	uint32_t flying;
+	uint32_t queued;
+};
+
 struct i915_scheduler {
 	struct list_head node_queue[I915_NUM_RINGS];
 	uint32_t flags[I915_NUM_RINGS];
 	spinlock_t lock;
 
+	/* Node counts: */
+	struct i915_scheduler_node_states counts[I915_NUM_RINGS];
+
 	/* Tuning parameters: */
 	int32_t priority_level_min;
 	int32_t priority_level_max;
