From b206019a7a0cdcf5b1926022843729444ae4195f Mon Sep 17 00:00:00 2001
From: John Harrison <John.C.Harrison@Intel.com>
Date: Wed, 2 Mar 2016 15:36:54 +0000
Subject: [PATCH 099/143] [VPG]: drm/i915/sched: fast dependency tracking

Add the requests to the context to simpilify the loop
---
 drivers/gpu/drm/i915/i915_drv.h         |    5 ++
 drivers/gpu/drm/i915/i915_gem.c         |    3 +
 drivers/gpu/drm/i915/i915_gem_context.c |    2 +
 drivers/gpu/drm/i915/i915_scheduler.c   |   90 +++++++++++++++++--------------
 drivers/gpu/drm/i915/i915_scheduler.h   |    3 +
 5 files changed, 62 insertions(+), 41 deletions(-)

Index: current/drivers/gpu/drm/i915/i915_drv.h
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_drv.h
+++ current/drivers/gpu/drm/i915/i915_drv.h
@@ -929,6 +929,7 @@ struct intel_context {
 	} engine[I915_NUM_RINGS];
 
 	struct list_head link;
+	struct list_head req_head;
 
 	/* perfmon configuration */
 	struct drm_i915_perfmon_context perfmon;
@@ -2225,6 +2226,8 @@ struct drm_i915_gem_object {
 	/** Breadcrumb of last fenced GPU access to the buffer. */
 	struct drm_i915_gem_request *last_fenced_req;
 
+	struct list_head req_head;
+
 	/** Current tiling stride for the object, if it's tiled. */
 	uint32_t stride;
 
@@ -2362,6 +2365,8 @@ struct drm_i915_gem_request {
 	/** Execlists no. of times this request has been sent to the ELSP */
 	int elsp_submitted;
 
+	struct list_head ctx_link;
+	uint32_t dep_uniq;
 };
 
 int i915_gem_request_alloc(struct intel_engine_cs *ring,
Index: current/drivers/gpu/drm/i915/i915_gem.c
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_gem.c
+++ current/drivers/gpu/drm/i915/i915_gem.c
@@ -3198,6 +3198,7 @@ int i915_gem_request_alloc(struct intel_
 		goto err;
 	}
 
+	INIT_LIST_HEAD(&req->ctx_link);
 	INIT_LIST_HEAD(&req->signal_link);
 	fence_init(&req->fence, &i915_gem_request_fops, &ring->fence_lock,
 		   ctx->engine[ring->id].fence_timeline.fence_context,
@@ -4983,6 +4984,8 @@ void i915_gem_object_init(struct drm_i91
 	obj->madv = I915_MADV_WILLNEED;
 
 	i915_gem_info_add_obj(obj->base.dev->dev_private, obj->base.size);
+
+	INIT_LIST_HEAD(&obj->req_head);
 }
 
 static const struct drm_i915_gem_object_ops i915_gem_object_ops = {
Index: current/drivers/gpu/drm/i915/i915_gem_context.c
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_gem_context.c
+++ current/drivers/gpu/drm/i915/i915_gem_context.c
@@ -222,6 +222,8 @@ __create_hw_context(struct drm_device *d
 	list_add_tail(&ctx->link, &dev_priv->context_list);
 	ctx->i915 = dev_priv;
 
+	INIT_LIST_HEAD(&ctx->req_head);
+
 	if (dev_priv->hw_context_size) {
 		struct drm_i915_gem_object *obj =
 				i915_gem_alloc_context_obj(dev, dev_priv->hw_context_size);
Index: current/drivers/gpu/drm/i915/i915_scheduler.c
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_scheduler.c
+++ current/drivers/gpu/drm/i915/i915_scheduler.c
@@ -757,53 +757,60 @@ static void i915_scheduler_file_queue_de
 	file_priv->scheduler_queue_length--;
 }
 
-static void i915_generate_dependencies(struct i915_scheduler *scheduler,
-				       struct i915_scheduler_queue_entry *node,
-				       uint32_t ring)
+static int i915_generate_dependencies(struct i915_scheduler *scheduler,
+				       struct i915_scheduler_queue_entry *node)
 {
-	struct i915_scheduler_obj_entry *this, *that;
-	struct i915_scheduler_queue_entry *test;
-	int i, j;
-	bool found;
+	uint32_t count = 0;
+
+	struct i915_scheduler_obj_entry *this_oe, *that_oe;
+	struct drm_i915_gem_request *req = node->params.request;
+	struct drm_i915_gem_request *that = node->params.request;
+	int i;
+
+	list_for_each_entry(that, &node->params.ctx->req_head, ctx_link) {
+		count++;
 
-	for_each_scheduler_node(test, ring) {
-		if (I915_SQS_IS_COMPLETE(test))
+		if (!that->scheduler_qe || I915_SQS_IS_COMPLETE(that->scheduler_qe))
 			continue;
 
-		/*
-		 * Batches on the same ring for the same
-		 * context must be kept in order.
-		 */
-		found = (node->params.ctx == test->params.ctx) &&
-			(node->params.ring == test->params.ring);
+		if (that->ring != node->params.ring)
+			continue;
 
-		/*
-		 * Batches working on the same objects must
-		 * be kept in order.
-		 */
-		for (i = 0; (i < node->num_objs) && !found; i++) {
-			this = node->objs + i;
+		if (that->dep_uniq != req->uniq) {
+			node->dep_list[node->num_deps] = that->scheduler_qe;
+			node->num_deps++;
+			that->dep_uniq = req->uniq;
+		}
+	}
 
-			for (j = 0; j < test->num_objs; j++) {
-				that = test->objs + j;
+	list_add_tail(&req->ctx_link, &node->params.ctx->req_head);
 
-				if (this->obj != that->obj)
-					continue;
+	for (i = 0; i < node->num_objs; i++) {
+		this_oe = node->objs + i;
 
-				/* Only need to worry about writes */
-				if (this->read_only && that->read_only)
-					continue;
+		list_for_each_entry(that_oe, &this_oe->obj->req_head, req_link) {
+			count++;
+			that = that_oe->req;
+
+			if (!that->scheduler_qe || I915_SQS_IS_COMPLETE(that->scheduler_qe))
+				continue;
 
-				found = true;
-				break;
+			/* Only need to worry about writes */
+			if (this_oe->read_only && that_oe->read_only)
+				continue;
+
+			if (that->dep_uniq != req->uniq) {
+				node->dep_list[node->num_deps] = that->scheduler_qe;
+				node->num_deps++;
+				that->dep_uniq = req->uniq;
 			}
 		}
 
-		if (found) {
-			node->dep_list[node->num_deps] = test;
-			node->num_deps++;
-		}
+		list_add_tail(&node->objs[i].req_link, &this_oe->obj->req_head);
+		node->objs[i].req = req;
 	}
+
+	return count;
 }
 
 static int i915_scheduler_queue_execbuffer_bypass(struct i915_scheduler_queue_entry *qe)
@@ -879,8 +886,9 @@ int i915_scheduler_queue_execbuffer(stru
 	struct intel_engine_cs *ring = qe->params.ring;
 	struct i915_scheduler_queue_entry *node;
 	bool not_flying;
-	int i, r;
+	int i;
 	int incomplete;
+	int count = 0;
 
 	if (qe->params.fence_wait)
 		scheduler->stats[ring->id].fence_got++;
@@ -933,12 +941,9 @@ int i915_scheduler_queue_execbuffer(stru
 	spin_lock_irq(&scheduler->lock);
 	node->num_deps = 0;
 
-	if (node->dep_list) {
-		for (r = 0; r < I915_NUM_RINGS; r++)
-			i915_generate_dependencies(scheduler, node, r);
+	count = i915_generate_dependencies(scheduler, node);
 
-		WARN_ON(node->num_deps > incomplete);
-	}
+	WARN_ON(node->num_deps > incomplete);
 
 	node->priority = clamp(node->priority,
 			       scheduler->priority_level_min,
@@ -1105,14 +1110,17 @@ void i915_scheduler_clean_node(struct i9
 	}
 
 	/* Release the locked buffers: */
-	for (i = 0; i < node->num_objs; i++)
+	for (i = 0; i < node->num_objs; i++) {
+		list_del(&node->objs[i].req_link);
 		drm_gem_object_unreference(&node->objs[i].obj->base);
+	}
 	kfree(node->objs);
 	node->objs = NULL;
 	node->num_objs = 0;
 
 	/* Context too: */
 	if (node->params.ctx) {
+		list_del(&node->params.request->ctx_link);
 		i915_gem_context_unreference(node->params.ctx);
 		node->params.ctx = NULL;
 	}
Index: current/drivers/gpu/drm/i915/i915_scheduler.h
===================================================================
--- current.orig/drivers/gpu/drm/i915/i915_scheduler.h
+++ current/drivers/gpu/drm/i915/i915_scheduler.h
@@ -53,6 +53,9 @@ const char *i915_scheduler_queue_status_
 struct i915_scheduler_obj_entry {
 	struct drm_i915_gem_object *obj;
 	bool read_only;
+
+	struct list_head req_link;
+	struct drm_i915_gem_request *req;
 };
 
 enum i915_scheduler_queue_entry_flags {
