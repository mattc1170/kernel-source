From: NeilBrown <neilb@suse.de>
Date: Thu, 24 Nov 2011 11:12:28 +1100
Subject: [PATCH 2/5] md/raid1 add failfast handling for reads.
References: Fate#311379
Git-commit: 2e52d449bcec31cb66d80aa8c798b15f76f1f5e0
Patch-mainline: v4.10

If a device is marked FailFast and it is not the only device
we can read from, we mark the bio as REQ_FAILFAST_DEV.

If this does fail, we don't try read repair but just allow
failure.
If it was the last device it doesn't fail of course so the retry
happens on the same device - this time without FAILFAST.  A subsequent
failure will not retry but will just pass up the error.

During resync we may use FAILFAST requests and on a failure we will
simply use the other device(s).

During recovery we will only use FAILFAST in the unusual case were
there are multiple places to read from - i.e. if there are > 2
devices.
If we get a failure we will fail the device and complete the
resync/recovery with remaining devices.

Signed-off-by: NeilBrown <neilb@suse.de>
---
 drivers/md/raid1.c |   65 +++++++++++++++++++++++++++++++++++++++++++----------
 drivers/md/raid1.h |    1 
 2 files changed, 54 insertions(+), 12 deletions(-)

--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@ -321,8 +321,10 @@ static void raid1_end_read_request(struc
 	struct r1bio *r1_bio = bio->bi_private;
 	int mirror;
 	struct r1conf *conf = r1_bio->mddev->private;
+	struct md_rdev *rdev;
 
 	mirror = r1_bio->read_disk;
+	rdev = conf->mirrors[mirror].rdev;
 	/*
 	 * this branch is our 'one mirror IO has finished' event handler:
 	 */
@@ -330,6 +332,11 @@ static void raid1_end_read_request(struc
 
 	if (uptodate)
 		set_bit(R1BIO_Uptodate, &r1_bio->state);
+	else if (test_bit(FailFast, &rdev->flags) &&
+		 test_bit(R1BIO_FailFast, &r1_bio->state))
+		/* This was a fail-fast read so we definitely
+		 * want to retry */
+		;
 	else {
 		/* If all other devices have failed, we want to return
 		 * the error upwards rather than fail the last device.
@@ -540,6 +547,7 @@ static int read_balance(struct r1conf *c
 	best_good_sectors = 0;
 	has_nonrot_disk = 0;
 	choose_next_idle = 0;
+	clear_bit(R1BIO_FailFast, &r1_bio->state);
 
 	if ((conf->mddev->recovery_cp < this_sector + sectors) ||
 	    (mddev_is_clustered(conf->mddev) &&
@@ -613,6 +621,10 @@ static int read_balance(struct r1conf *c
 		} else
 			best_good_sectors = sectors;
 
+		if (best_disk >= 0)
+			/* At least two disks to choose from so failfast is OK */
+			set_bit(R1BIO_FailFast, &r1_bio->state);
+
 		nonrot = blk_queue_nonrot(bdev_get_queue(rdev->bdev));
 		has_nonrot_disk |= nonrot;
 		pending = atomic_read(&rdev->nr_pending);
@@ -652,7 +664,8 @@ static int read_balance(struct r1conf *c
 			break;
 		}
 		/* If device is idle, use it */
-		if (pending == 0) {
+		if (pending == 0 &&
+		    (!test_bit(FailFast, &rdev->flags) || best_disk >= 0)) {
 			best_disk = disk;
 			break;
 		}
@@ -678,7 +691,7 @@ static int read_balance(struct r1conf *c
 	 * mixed ratation/non-rotational disks depending on workload.
 	 */
 	if (best_disk == -1) {
-		if (has_nonrot_disk)
+		if (has_nonrot_disk || min_pending == 0)
 			best_disk = best_pending_disk;
 		else
 			best_disk = best_dist_disk;
@@ -1167,6 +1180,9 @@ read_again:
 		read_bio->bi_bdev = mirror->rdev->bdev;
 		read_bio->bi_end_io = raid1_end_read_request;
 		read_bio->bi_rw = READ | do_sync;
+		if (test_bit(FailFast, &mirror->rdev->flags) &&
+		    test_bit(R1BIO_FailFast, &r1_bio->state))
+			read_bio->bi_rw |= REQ_FAILFAST_DEV;
 		read_bio->bi_private = r1_bio;
 
 		if (max_sectors < r1_bio->sectors) {
@@ -1451,6 +1467,7 @@ static void error(struct mddev *mddev, s
 	 * next level up know.
 	 * else mark the drive as failed
 	 */
+	spin_lock_irqsave(&conf->device_lock, flags);
 	if (test_bit(In_sync, &rdev->flags)
 	    && (conf->raid_disks - mddev->degraded) == 1) {
 		/*
@@ -1460,20 +1477,20 @@ static void error(struct mddev *mddev, s
 		 * it is very likely to fail.
 		 */
 		conf->recovery_disabled = mddev->recovery_disabled;
+		spin_unlock_irqrestore(&conf->device_lock, flags);
 		return;
 	}
 	set_bit(Blocked, &rdev->flags);
-	spin_lock_irqsave(&conf->device_lock, flags);
 	if (test_and_clear_bit(In_sync, &rdev->flags)) {
 		mddev->degraded++;
 		set_bit(Faulty, &rdev->flags);
 	} else
 		set_bit(Faulty, &rdev->flags);
-	spin_unlock_irqrestore(&conf->device_lock, flags);
 	/*
 	 * if recovery is running, make sure it aborts.
 	 */
 	set_bit(MD_RECOVERY_INTR, &mddev->recovery);
+	spin_unlock_irqrestore(&conf->device_lock, flags);
 	set_bit(MD_CHANGE_DEVS, &mddev->flags);
 	set_bit(MD_CHANGE_PENDING, &mddev->flags);
 	printk(KERN_ALERT
@@ -1806,12 +1823,24 @@ static int fix_sync_read_error(struct r1
 	sector_t sect = r1_bio->sector;
 	int sectors = r1_bio->sectors;
 	int idx = 0;
+	struct md_rdev *rdev;
+
+	rdev = conf->mirrors[r1_bio->read_disk].rdev;
+	if (test_bit(FailFast, &rdev->flags)) {
+		/* Don't try recovering from here - just fail it
+		 * ... unless it is the last working device of course */
+		md_error(mddev, rdev);
+		if (test_bit(Faulty, &rdev->flags))
+			/* Don't try to read from here, but make sure
+			 * put_buf does it's thing
+			 */
+			bio->bi_end_io = end_sync_write;
+	}
 
 	while(sectors) {
 		int s = sectors;
 		int d = r1_bio->read_disk;
 		int success = 0;
-		struct md_rdev *rdev;
 		int start;
 
 		if (s > (PAGE_SIZE>>9))
@@ -2301,16 +2330,20 @@ static void handle_read_error(struct r1c
 	 * This is all done synchronously while the array is
 	 * frozen
 	 */
-	if (mddev->ro == 0) {
+	rdev = conf->mirrors[r1_bio->read_disk].rdev;
+	if (mddev->ro == 0
+	    && !test_bit(FailFast, &rdev->flags)) {
 		freeze_array(conf, 1);
 		fix_read_error(conf, r1_bio->read_disk,
 			       r1_bio->sector, r1_bio->sectors);
 		unfreeze_array(conf);
 	} else
-		md_error(mddev, conf->mirrors[r1_bio->read_disk].rdev);
-	rdev_dec_pending(conf->mirrors[r1_bio->read_disk].rdev, conf->mddev);
+		md_error(mddev, rdev);
+	rdev_dec_pending(rdev, conf->mddev);
 
 	bio = r1_bio->bios[r1_bio->read_disk];
+	r1_bio->bios[r1_bio->read_disk] =
+		mddev->ro ? IO_BLOCKED : NULL;
 	bdevname(bio->bi_bdev, b);
 read_more:
 	disk = read_balance(conf, r1_bio, &max_sectors);
@@ -2318,15 +2351,14 @@ read_more:
 		printk(KERN_ALERT "md/raid1:%s: %s: unrecoverable I/O"
 		       " read error for block %llu\n",
 		       mdname(mddev), b, (unsigned long long)r1_bio->sector);
+		if (bio)
+			bio_put(bio);
 		raid_end_bio_io(r1_bio);
 	} else {
 		const unsigned long do_sync
 			= r1_bio->master_bio->bi_rw & REQ_SYNC;
-		if (bio) {
-			r1_bio->bios[r1_bio->read_disk] =
-				mddev->ro ? IO_BLOCKED : NULL;
+		if (bio)
 			bio_put(bio);
-		}
 		r1_bio->read_disk = disk;
 		bio = bio_clone_mddev(r1_bio->master_bio, GFP_NOIO, mddev);
 		bio_trim(bio, r1_bio->sector - bio->bi_iter.bi_sector,
@@ -2343,6 +2375,9 @@ read_more:
 		bio->bi_bdev = rdev->bdev;
 		bio->bi_end_io = raid1_end_read_request;
 		bio->bi_rw = READ | do_sync;
+		if (test_bit(FailFast, &rdev->flags) &&
+		    test_bit(R1BIO_FailFast, &r1_bio->state))
+			bio->bi_rw |= REQ_FAILFAST_DEV;
 		bio->bi_private = r1_bio;
 		if (max_sectors < r1_bio->sectors) {
 			/* Drat - have to split this up more */
@@ -2616,6 +2651,8 @@ static sector_t sync_request(struct mdde
 			bio->bi_iter.bi_sector = sector_nr + rdev->data_offset;
 			bio->bi_bdev = rdev->bdev;
 			bio->bi_private = r1_bio;
+			if (test_bit(FailFast, &rdev->flags))
+				bio->bi_rw |= REQ_FAILFAST_DEV;
 		}
 	}
 	rcu_read_unlock();
@@ -2747,6 +2784,8 @@ static sector_t sync_request(struct mdde
 			if (bio->bi_end_io == end_sync_read) {
 				read_targets--;
 				md_sync_acct(bio->bi_bdev, nr_sectors);
+				if (read_targets == 1)
+					bio->bi_rw &= ~REQ_FAILFAST_MASK;
 				generic_make_request(bio);
 			}
 		}
@@ -2754,6 +2793,8 @@ static sector_t sync_request(struct mdde
 		atomic_set(&r1_bio->remaining, 1);
 		bio = r1_bio->bios[r1_bio->read_disk];
 		md_sync_acct(bio->bi_bdev, nr_sectors);
+		if (read_targets == 1)
+			bio->bi_rw &= ~REQ_FAILFAST_MASK;
 		generic_make_request(bio);
 
 	}
--- a/drivers/md/raid1.h
+++ b/drivers/md/raid1.h
@@ -182,4 +182,5 @@ struct r1bio {
  */
 #define	R1BIO_MadeGood 7
 #define	R1BIO_WriteError 8
+#define	R1BIO_FailFast 9
 #endif
